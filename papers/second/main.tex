\documentclass[conference]{IEEEtran}

\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
   \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.eps, .ps}
\fi

\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{url}
\usepackage{listings}
\lstset{ %
language=Python,                % choose the language of the code
basicstyle=\ttfamily\footnotesize,  % the size of the fonts that are used for the code
keywordstyle=\bfseries,
frame=single,	                % adds a frame around the code
tabsize=4,   	                % sets default tabsize to 2 spaces
captionpos=b,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)},          % if you want to add a comment within your code
xleftmargin=5mm,                % indent listing slightly to get line numbers back onto page
xrightmargin=5mm,
numbers=left, numberstyle=\ttfamily\bfseries\footnotesize
}

\hyphenation{}


\begin{document}

\title{Automatic latency hiding for Distributed Numerical Python by means of lazy evaluation}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Mads Ruben Burgdorff Kristensen}
\IEEEauthorblockA{eScience Centre\\
University of Copenhagen\\
Denmark}
\and
\IEEEauthorblockN{Brian Vinter}
\IEEEauthorblockA{eScience Centre\\
University of Copenhagen\\
Denmark}}

% make the title area
\maketitle

\begin{abstract}

\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
In this paper we introduce generic latency hiding for Distributed Numerical Python (DistNumPy)\cite{distnumpy09}, which is a library for doing numerical computation in Python that targets scalable distributed memory architectures. DistNumPy extends the NumPy module\cite{numpy}, which is popular for scientific programming. Replacing NumPy with DistNumPy enables the user to write sequential Python programs that seamlessly utilize distributed memory architectures.

Communication latency hiding is well known technique to improve the performance and scalability of communication bound problems. We define latency hiding informally as in \cite{Strumpen94latencyhiding} -- ``a technique to increase processor utilization by transferring data via the network while continuing with the computation at the same time''

There exist a lot of methods to implement latency hiding and most of them are tailored to a particular problem. The current method used in DistNumPy to hide the communication latency is also tailored to a particular problem -- namely an element-by-element computation on distributed arrays that uses identical distribution schemes. However, a broad range of DistNumPy programs make use of distributed arrays that do not share identical distribution schemes. In this paper we therefore introduce a generic method for latency hiding that is not tailored to any particular problem.

\subsection{Motivation}
\subsection{Related work}

\section{Distributed Numerical Python}
The programming language Python combined with the numerical library NumPy\cite{numpy}  has become a popular numerical framework amongst researchers. It offers a high level programming language to implement new algorithms that support a broad range of high-level operations directly on vectors and matrices.

The idea in NumPy is to provide a numerical extension to the Python language that enables the Python language to be both high productive and high performing. NumPy provides not only an API to standardized numerical solvers, but a possibility to develop new numerical solvers that are both implemented and efficiently executed in Python, much like the idea behind the MATLAB\cite{guide1998mathworks} framework. 

DistNumPy is a new version of NumPy that parallelizes array operations in a manner completely transparent to the user -- from the perspective of the user, the difference between NumPy and DistNumPy is minimal. DistNumPy can use multiple processors through the communication library Message Passing Interface (MPI)\cite{mpi}. However, DistNumPy do not use the traditional SPMD parallel programming model because it requires the user to differentiate between the MPI-processes. Instead the MPI communication in DistNumPy is fully transparent and the user needs no knowledge of MPI or any parallel programming model. 
The only difference in the API of NumPy and DistNumPy is the array creation routines. DistNumPy allow both distributed and non-distributed arrays to co-exist thus the user must specify, as an optional parameter, if the array should be distributed. The following illustrates the only difference between the creation of a standard array and a distributed array:
\lstset{frame=none, xleftmargin=0mm, numbers=none}
\begin{lstlisting}
#Non-Distributed
A = numpy.array([1,2,3])
#Distributed
B = numpy.array([1,2,3], dist=True)
\end{lstlisting}
\lstset{frame=single, xleftmargin=5mm, numbers=left}



\section{Data Layout}
Short introduction to the data layout and views.


\section{Lazy Evaluation}




Lazy evaluation on these data blocks are managed through a simple dependency list for each block. When a NumPy operation is issued it is split across the sub-view blocks that are involved in the operation. For each such operation on a sub-view block a set of tasks are created, one for each combined set of data-blocks to sub-view blocks. Dependencies are then added to the dependency-list of each data-block that is involved, each dependency link back to the individual tasks. When the number of dependencies on a task reached zero the task may be moved to the ready-queue. But before this is done we traverse the dependency list and include any other task that can be merged into the ready task, given that they could execute in parallel with the task or could execute given the task was done, when two tasks are merged into one, the remaining list is traversed under the same rules but for the merged task.


\subsection{Apply Operation List}




\section{Examples}


\section{Experiments}


\section{Future work}


\section{Conclusions}


\bibliographystyle{IEEEtran}
\bibliography{/home/madsbk/repos/priv/diku/phd/paper_archive/main}

\end{document}