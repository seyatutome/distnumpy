\documentclass[conference]{IEEEtran}

\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
   \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
   \DeclareGraphicsExtensions{.eps, .ps}
\fi

\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{url}
\usepackage{listings}
\lstset{ %
language=Python,                % choose the language of the code
basicstyle=\ttfamily\footnotesize,  % the size of the fonts that are used for the code
keywordstyle=\bfseries,
frame=single,	                % adds a frame around the code
tabsize=4,   	                % sets default tabsize to 2 spaces
captionpos=b,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
escapeinside={\%*}{*)},          % if you want to add a comment within your code
xleftmargin=5mm,                % indent listing slightly to get line numbers back onto page
xrightmargin=5mm,
numbers=left, numberstyle=\ttfamily\bfseries\footnotesize
}

\hyphenation{}


\begin{document}

\title{Generic Latency Hiding for Distributed Numerical Python by Means of Lazy Evaluation}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Mads Ruben Burgdorff Kristensen}
\IEEEauthorblockA{eScience Centre\\
University of Copenhagen\\
Denmark}
\and
\IEEEauthorblockN{Brian Vinter}
\IEEEauthorblockA{eScience Centre\\
University of Copenhagen\\
Denmark}}

% make the title area
\maketitle

\begin{abstract}

\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
In this paper we introduce generic latency hiding for Distributed Numerical Python (DistNumPy)\cite{distnumpy09}, which is a library for doing numerical computation in Python that targets scalable distributed memory architectures. DistNumPy extends the NumPy module\cite{numpy}, which is popular for scientific programming. Replacing NumPy with DistNumPy enables the user to write sequential Python programs that seamlessly utilize distributed memory architectures.

Communication latency hiding is well known technique to improve the performance and scalability of communication bound problems. We define latency hiding informally as in \cite{Strumpen94latencyhiding} -- ``a technique to increase processor utilization by transferring data via the network while continuing with the computation at the same time''.

There exist a lot of methods to implement latency hiding and most of them are tailored to a particular problem. The current method used in DistNumPy to hide the communication latency is also tailored to a particular problem -- namely an element-by-element computation on distributed arrays that uses identical distribution schemes. However, a broad range of DistNumPy programs make use of distributed arrays that do not share identical distribution schemes. In this paper we therefore introduce a generic method for latency hiding that is not tailored to any particular problem.

\subsection{Motivation}
\subsection{Related work}

\section{Distributed Numerical Python}
The programming language Python combined with the numerical library NumPy\cite{numpy} has become a popular numerical framework amongst researchers. It offers a high level programming language to implement new algorithms that support a broad range of high-level operations directly on vectors and matrices.

The idea in NumPy is to provide a numerical extension to the Python language that enables the Python language to be both high productive and high performing. NumPy provides not only an API to standardized numerical solvers, but a possibility to develop new numerical solvers that are both implemented and efficiently executed in Python, much like the idea behind the MATLAB\cite{guide1998mathworks} framework. 

DistNumPy is a new version of NumPy that parallelizes array operations in a manner completely transparent to the user -- from the perspective of the user, the difference between NumPy and DistNumPy is minimal. DistNumPy can use multiple processors through the communication library Message Passing Interface (MPI)\cite{mpi}. However, DistNumPy do not use the traditional SPMD parallel programming model because it requires the user to differentiate between the MPI-processes. Instead the MPI communication in DistNumPy is fully transparent and the user needs no knowledge of MPI or any parallel programming model. 
The only difference in the API of NumPy and DistNumPy is the array creation routines. DistNumPy allow both distributed and non-distributed arrays to co-exist thus the user must specify, as an optional parameter, if the array should be distributed. The following illustrates the only difference between the creation of a standard array and a distributed array:
\lstset{frame=none, xleftmargin=0mm, numbers=none}
\begin{lstlisting}
#Non-Distributed
A = numpy.array([1,2,3])
#Distributed
B = numpy.array([1,2,3], dist=True)
\end{lstlisting}
\lstset{frame=single, xleftmargin=5mm, numbers=left}


\section{Data Layout}
The data layout used in DistNumPy consists of three levels (Fig. \ref{fig:view_block}). The first level is the base-block level, in which a base-block map directly into one block of memory located on one node. The memory block is not necessarily contiguous but only one MPI-process has exclusive access to the block. A base-block is the basic build block in the distribution scheme used when distributing DistNumPy arrays across multiple MPI-processes. The distribution scheme used in DistNumPy is the N-Dimensional Block Cyclic Distribution\cite{distnumpy09} inspired by High Performance Fortran\cite{Loveman93}. 

The second level is the view-block level, which is an abstraction level above the base-block level. A view-block represents a basic block of an array-view, which means that a view-block is a contiguous block of array elements from the perspective of the user. A view-block can span over multiple base-blocks and consequently also over multiple MPI-processes. For a MPI-process to access a whole view-block it will have to fetch data from possible remote MPI-processes and put the pieces together before accessing the block. To avoid this process, which may require some internal memory copying, we divide view-blocks into sub-view-block.

Sub-view-block is the third block level and is simply a block of data that is a part of a view-block but is located on only one MPI-process. The general idea is that all array operation is translated into a number of sub-view-block operations.

We will define a \emph{aligned array} as an array that have a direct contiguous mapping through the block hierarchy. That is, a distributed array in which the base-blocks, view-blocks and sub-view-blocks are identical. A \emph{non-aligned array} is then a distributed array without this property.

\begin{figure}
 \centering
 \includegraphics[width=150px]{gfx/view_blocks}
 \caption{An illustration of the block hierarchy used in representing a 2D distributed array. The array is divided into three block-types: Base, View and Sub-View blocks. The 16 base-blocks make up the base-array, which may be distributed between multiple MPI-processes. The 9 view-blocks make up a view of the base-array and represent the elements that are visible to the user. Each view-block is furthermore divided into four sub-view-blocks, which are the basic block of elements and is always located on one MPI-process.}
 \label{fig:view_block}
\end{figure}


\section{Universal Function}
An important mechanism in DistNumPy is a concept called Universal function. A universal function (ufunc) is a function that operates on all elements in an array independently. That is, a ufunc is a vectorized wrapper for a function that takes a fixed number of scalar inputs and produces a fixed number of scalar outputs. Using ufunc can result in a significant performance boost compared to native Python because the computation-loop is implemented in C and is executed in parallel. 

An ufunc operation is translated into a number of view-block operations which again is translated into a number of sub-view-block operations. 


Ufuncs are parallelized by the data distribution of the output array.

\subsection{Dispatching}
The MPI-process hierarchy in DistNumPy has one MPI-process (master) placed above the others (slaves). All MPI-processes run the Python interpreter but only the master executes the user-program, the slaves will block at the \texttt{import numpy} statement. 

The following describes the flow of the dispatching:
\begin{enumerate}
\item The master is the dispatcher and will, when the user applies a python command on a distributed array, compose a message with meta-data describing the command. 
\item The message is then broadcasted from the master to the slaves with a blocking MPI-broadcast. It is important to note that the message only contains meta-data and not any actual array data.
\item After the broadcast, all MPI-processes will apply the command on the sub-array they own and exchange array elements as required (Point-to-Point communication).
\item When the command is completed, the slaves will wait for the next command from the master and the master will return to the user's python program. The master will return even though some slaves may still be working on the command, synchronization is therefore required before the next command broadcast.
\end{enumerate}



\section{Latency Hiding}
\begin{figure}
 \centering
 \includegraphics[width=200px]{gfx/double_buffering}
 \caption{Flow diagram illustrating double buffering. The $n$'th iteration is expressed with a $n$ and Comm and Comp represents communication and computation, respectively. $n$++ is an iteration to $n$'s successor.}
 \label{fig:double_buffering}
\end{figure}

The standard approach to hide communication latency behind computation in message-passing is a technique known as double buffering. The implementation of double buffering is straightforward when operating on a set of data block that all have identical sizes. The communication of one data block is overlapped with the computation of another already communicated data block (Fig. \ref{fig:double_buffering}) and since the sizes of all the data blocks are identical all iterations are also identical.

The straightforward double buffering approach is used in the current version of DistNumPy. It works very well for ufuncs that operates on aligned arrays because it translates into one communication and computation operation per data block. However, for ufuncs that operates on non-aligned arrays the approach is inadequate because it translates into a distributed depended number of communication and computation operation per data block.


\section{Lazy Evaluation}


Lazy evaluation on these data blocks are managed through a simple dependency list for each block. When a NumPy operation is issued it is split across the sub-view blocks that are involved in the operation. For each such operation on a sub-view block a set of tasks are created, one for each combined set of data-blocks to sub-view blocks. Dependencies are then added to the dependency-list of each data-block that is involved, each dependency link back to the individual tasks. When the number of dependencies on a task reached zero the task may be moved to the ready-queue. But before this is done we traverse the dependency list and include any other task that can be merged into the ready task, given that they could execute in parallel with the task or could execute given the task was done, when two tasks are merged into one, the remaining list is traversed under the same rules but for the merged task.


\subsection{Apply Operation List}




\section{Examples}


\section{Experiments}


\section{Future work}


\section{Conclusions}


\bibliographystyle{IEEEtran}
\bibliography{/home/madsbk/repos/priv/diku/phd/paper_archive/main}

\end{document}