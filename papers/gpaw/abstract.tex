\documentclass[a4paper,10pt]{article}
\usepackage[margin=2cm]{geometry}


%opening
\title{GPAW optimized for Blue Gene/P \\ using hybrid programming}
%\date{}


\author{Mads Ruben Burgdorff Kristensen \and Hans Henrik Happe \and Brian Vinter}
\begin{document}


\maketitle
The concept of massively parallel processors has been taken to the extreme with the introduction of the BlueGene architectures from IBM. With hundreds of thousands of processors in one machine the parallelism is extreme, but so are the techniques that must be applied to obtain performance with that many processors.

The current trend in HPC hardware is towards clusters of shared-memory computation nodes. The Blue Gene/P also follows this trend and consists of four CPU-cores per node. Furthermore it is quite possible that future versions of the Blue Gene architecture will consists of even more CPU-cores per node.

To exploit the memory locality in shared-memory computation nodes a paradigm which combines shared and distributed memory programming may be of interest. The idea is to avoid communication between CPU-cores on the same node. Unfortunately, it is not trivial to obtain good performance when combining shared-memory programming with distributed memory programming. Even though some internal communication is avoided, it is often the case that the sole use of MPI\cite{mpi} outperforms a combination of threads and MPI when computing on clusters of shared-memory computation nodes\cite{henty2000, hipp04Hybrid, VinterB}.

In this work we present optimizations of a Grid-based projector-augmented wave method software, GPAW \cite{Mortensen05} for the Blue Gene/P architecture. The improvements are achieved by using communication techniques suitable for the Blue Gene/P architecture and a programming paradigm which combine shared and distributed memory programming known as hybrid programming.

However for GPAW, hybrid programming have an additional advantage. The nature of GPAW means that a reduction in the number of MPI-processes will produce coarser grained memory decomposition. As a consequence the inter-node communication will therefore be reduced when a hybrid programming technique is used.

The presented work focuses on optimizing a very time consuming operation in GPAW, the finite-different stencil operation. GPAW uses a fine-grained memory decomposition, which impose a challenge for the parallelization of the finite-different stencil operation. The problem is that the parallelization becomes very fine grained and good scalability becomes hard to obtain. It is in this context that the Blue Gene/P has its strength. The ratio between computation power, memory bandwidth and network bandwidth is very balanced on the Blue Gene/P. Accessing data, in particular remote data, impose a limited penalty and good scalability may therefore be possible even though the parallelization is fine grained.

We evaluate two different hybrid programming approaches. One approach in which inter-node communication is handled individually by every thread and another approach in which one thread handles the inter-node communication on behalf of all the other threads in a node. The work shows that, on the Blue Gene/P, the first approach is clearly superior the latter. In \cite{Cappello2000} the authors concludes that, on a well balanced system, a loop level parallelization approach, corresponding to our second hybrid approach, is unfavorably compared to a strictly MPI implementation. Our first hybrid approach was developed on the basis of that conclusion.

Different communication techniques are also explained and evaluated. The most significant performance improvement derives from double buffering and communication batching. Since Blue Gene/P is such a well-balanced system it is possible to do a lot of latency hiding with those two techniques.

The work succeeds in scaling the finite-different stencil operation up to 16384 CPU-cores by using double buffering and communication batching. Furthermore we demonstrate a hybrid programming model which is clearly beneficial compared to the original flat programming model. In total an improvement of 1.94 compared to the original implementation is obtained. The results we demonstrate are reasonably general and may be applied to other finite difference codes.


\bibliographystyle{IEEEtran}
\bibliography{/home/madsbk/repos/priv/diku/phd/paper_archive/main}

\end{document}